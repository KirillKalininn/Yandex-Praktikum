{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основные проблемы, с которыми придется столкнуться:\n",
    "* <b>сервер выдает только первые 200 записей</b>, соответствующих запросу, даже если их 200 000. Это значит, что данные придется получать кусками по 200 строк, эти 200 строк записывать, вводить новый запрос, заново нажимать кнопку, получать новые 200 строк и опять писать их в файл. Поэтому процесс затягивается.\n",
    "* <b>время отработки запроса сервером</b>. Оно разнится от 0,2 секунд до 5 секунд (>5 секунд - timeout, про это в следующем пункте). Это ведет к тому, что у нас есть 2 варианта, как разворачивать программу:\n",
    "а) или на каждой итерации ждать ровно 5 секунд в ожидании, пока программа 100% отработает запрос;\n",
    "б) или ждать, предположим, 1 секунду, потом проверять, отработан ли запрос, если нет, то ждать ещё 1 секунду и опять проверять, и так до победного (выбираем этот вариант, т.к. мы сэкономим по 4 секунды на запросах, которые отработались моментально и сэкономим максимум из возможного времени, проверяя результат каждые 1-2 секунды, вместо всех 5).\n",
    "* <b>timeout</b> - то, что вываливается, когда сервер не смог в отведенные 5000 мс отработать запрос. По моим наблюдениям, это происходит нередко тогда, когда мы парсим данные в 2 потока (например, 1 поток парсит данные, начиная с начала таблицы, 2 - начиная с конца), тогда, когда оба потока в один момент отправляют запрос в базу данных, база данных и так медленно их отрабатывает, а их тут ещё и два, всё вываливается в timeout. При однопоточном режиме такие ситуации практически исключены. Остановился на варианте с одним потоком, так как два потока не дают практически никакого прироста производительности, но увеличивают риски нарваться на timeout, а это ведёт к увеличению среднего времени работы по каждой итерации. Тем не менее, нам необходимо предусмотреть эту ситуацию.\n",
    "<br/><br/><br/>\n",
    "Как выглядит процесс:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установим (если не установлена) библиотеку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver-manager\n",
      "  Downloading https://files.pythonhosted.org/packages/c2/f6/bd9d73991fd8d1b488aefa4b55711af670e938a9b8549ffd8ecc42780069/webdriver_manager-2.3.0-py2.py3-none-any.whl\n",
      "Collecting configparser (from webdriver-manager)\n",
      "  Downloading https://files.pythonhosted.org/packages/7a/2a/95ed0501cf5d8709490b1d3a3f9b5cf340da6c433f896bbe9ce08dbe6785/configparser-4.0.2-py2.py3-none-any.whl\n",
      "Collecting crayons (from webdriver-manager)\n",
      "  Downloading https://files.pythonhosted.org/packages/f8/64/ab71c69db049a5f404f1f2c7627578f4b59aca55e6ad9d939721ce6466dd/crayons-0.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests in e:\\users\\higem\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.22.0)\n",
      "Requirement already satisfied: colorama in e:\\users\\higem\\anaconda3\\lib\\site-packages (from crayons->webdriver-manager) (0.4.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in e:\\users\\higem\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in e:\\users\\higem\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\users\\higem\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in e:\\users\\higem\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.0.4)\n",
      "Installing collected packages: configparser, crayons, webdriver-manager\n",
      "Successfully installed configparser-4.0.2 crayons-0.3.0 webdriver-manager-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "\n",
    "caps = webdriver.DesiredCapabilities().FIREFOX\n",
    "caps[\"marionette\"] = True\n",
    "browser1 = webdriver.Firefox(executable_path=r'E:\\Chrome\\geckodriver.exe', capabilities=caps)\n",
    "browser1.get(\"https://praktikum.yandex.ru/trainer/data-analyst/lesson/9bd17813-3d5c-47da-ab3c-097baced1979/task/03fbb4b3-ba91-4660-a05e-7b181f597407/\")\n",
    "\n",
    "# для ввода логина-пароля и очистки поля ввода запроса\n",
    "time.sleep(15)\n",
    "\n",
    "# ищем поле ввода sql-запроса и кнопку \"play\"\n",
    "inputElement1 = browser1.find_elements_by_tag_name('textarea')[0]\n",
    "button1 = browser1.find_elements_by_xpath(\"//button[@class='button button_has-hover-color button_size_m button_type_icon button_theme_light button_view_clear']\")[0]\n",
    "\n",
    "# пишем первый запрос и спим на всякий случай \n",
    "# (БД иногда плохо реагирует на почти одновременный ввод запроса и нажатие кнопки)\n",
    "inputElement1.send_keys(\"select *\\nfrom flights\")\n",
    "time.sleep(1)\n",
    "\n",
    "# проверяем, есть ли вывод (когда вывод не подгрузился - кнопка ещё не активна)\n",
    "# кнопку на \"активность\" и проверяем\n",
    "try:\n",
    "    button1.click()\n",
    "except:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        button1.click()\n",
    "    except:\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            button1.click()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "# ждем вывода результата    \n",
    "time.sleep(5)\n",
    "\n",
    "# парсим данные и ищем таблицу с результатом\n",
    "parser1 = BeautifulSoup(browser1.page_source,\"lxml\")\n",
    "table1 = parser1.find(\"table\")\n",
    "\n",
    "# пишем в файл\n",
    "with open('flights.csv','w+', newline='') as csvfile1:\n",
    "    writer1 = csv.writer(csvfile1, delimiter=',')\n",
    "    \n",
    "    for tr in table1.findAll(\"tr\")[1:]:\n",
    "        list_of_cells1 = list()\n",
    "        for td in tr.findAll(\"td\"):\n",
    "            list_of_cells1.append(td.text)\n",
    "        writer1.writerow(list_of_cells1)\n",
    "\n",
    "# количество циклов в зависимости от общего количества строк в БД    \n",
    "len_of_db = 65664 // 200\n",
    "\n",
    "# запускаем цикл для ввода sql-запросов уже с offset и парсинга результатов\n",
    "for i in tqdm_notebook(range(len_of_db)):\n",
    "    # сохраняем таблицу, спарсенную в предыдущей итерации (для последующей сверки)\n",
    "    prev_table1 = table1\n",
    "    # удаляем последние символы после offset в определенном количестве\n",
    "    if i >= 1:\n",
    "        chars_to_delete = len(str(i*200))\n",
    "        for delete_count in range(chars_to_delete):\n",
    "            inputElement1.send_keys(Keys.BACKSPACE) \n",
    "    i += 1\n",
    "    # дополняем sql-запрос\n",
    "    if i > 1:\n",
    "        inputElement1.send_keys(\"{}\".format(i*200))\n",
    "    else:\n",
    "        inputElement1.send_keys(\"\\noffset {}\".format(i*200))\n",
    "        \n",
    "    time.sleep(0.5)\n",
    "    # нажимаем кнопку для получения результата \n",
    "    # (иногда это получается не с первого раза, поэтому try-except)\n",
    "    try:\n",
    "        button1.click()\n",
    "    except:\n",
    "        try:\n",
    "            time.sleep(0.5)\n",
    "            button1.click()\n",
    "        except:\n",
    "            try:\n",
    "                time.sleep(0.5)\n",
    "                button1.click()\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # спим в ожидании вывода результата\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # проверка, точно ли нам выдались результаты \n",
    "    # (если \"нет\", кнопка будет неактивна, если \"да\", то уже сохраненный в кэш резуьтат будет выведен заново моментально)\n",
    "    try:\n",
    "        button1.click()\n",
    "    except:\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            button1.click()\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            try:\n",
    "                time.sleep(1)\n",
    "                button1.click()\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    # получаем таблицу с данными\n",
    "    parser1 = BeautifulSoup(browser1.page_source,\"lxml\")\n",
    "    table1 = parser1.find(\"table\")\n",
    "    # счетчики, сколько раз мы спарсили ту же таблицу (долго грузилась новая) и сколько раз был timeout\n",
    "    prev_counter = 0\n",
    "    none_counter = 0\n",
    "    \n",
    "    # цикл \"до победного\" (до получения нужной таблицы)\n",
    "    # на случай, если оказалось, что мы спарсили ту же таблицу, что и в итерации ранее,\n",
    "    # или когда у нас timeout, и таблица - пустой объект:\n",
    "    while table1 == prev_table1 or table1 is None:\n",
    "        if table1 == prev_table1:\n",
    "            reason = 'повтор'\n",
    "            prev_counter += 1\n",
    "        elif table1 is None:\n",
    "            reason = 'timeout'\n",
    "            none_counter += 1\n",
    "            \n",
    "        print('Я тут, причина - {} в {} раз'.format(reason, [none_counter, prev_counter][reason == 'повтор']))\n",
    "        \n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            button1.click()\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            try:\n",
    "                time.sleep(1)\n",
    "                button1.click()\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                pass\n",
    "        parser1 = BeautifulSoup(browser1.page_source,\"lxml\")\n",
    "        table1 = parser1.find(\"table\")\n",
    "    \n",
    "    # записываем очередную порцию данных в файл\n",
    "    with open('flights.csv','a', newline='') as csvfile1:\n",
    "        writer1 = csv.writer(csvfile1, delimiter=',')\n",
    "\n",
    "        for tr in table1.findAll(\"tr\")[1:]:\n",
    "            list_of_cells1 = list()\n",
    "            for td in tr.findAll(\"td\"):\n",
    "                list_of_cells1.append(td.text)\n",
    "            writer1.writerow(list_of_cells1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
